---
title: My Journey with Large Language Models
description: put a desc here
date: "2023-09-05"
authors:
  - beneverman
---

<Callout type='warning'>All code is [here](https://github.com/beverm2391). Want a quick read? <a className='font-medium underline underline-offset-4 dark:text-white text-slate-800' href='#level-3'>Skip to the coolest stuff</a>.</Callout>

My first ever Python script was a cron job that periodically emailed me new PubMed articles related to a medicine I was on. My second was a script that sent me a push notification every time a piece of rare climbing gear was posted for sale on Mountain Project. In early 2022, I read GPT-3’s inaugural paper, Language Models Are Few Shot Learners (2020), and was awakened to the potential of leveraging Large Language Models (LLMs) to extend the complexity and capability of my projects. Here I’m going to lay out a timeline of the projects I’ve built using LLMs - their successes, limitations, and general usefulness today. Most of my code has been gradually built into a package called benlp which I’ll link (now) and again later. 

All of these projects leverage OpenAI’s API. The nice thing about building on top of their models is that OpenAI is constantly innovating and improving model performance, which typically improves my project performance. See the <a className='font-medium underline underline-offset-4 dark:text-white text-slate-800' href='#oai-timeline'>OpenAI Release Timeline</a> for a list of major releases.

## My Projects

### Level 1 - Out Of The Box

Before GPT-3 was instruction tuned (figure out which model this would have been), the lowest hanging fruit in 2022 was using (model name) to assist in writing for my classes. At this point, the model was unable to write coherently out of the box, even with precise prompting. Then I wrote a script that would take a PDF or PowerPoint and generate bulleted notes. After I built this, I realized that I could code during all my lectures and just work off my generated summaries. Then I experimented with outputting notes in a format that I could batch import to Quizlet flashcards. At the time the best model was text-davinci-002, which used [supervised fine-tuning but not reinforcement learning](https://arxiv.org/abs/2303.10420) so getting a consistently formatted output was difficult. 

### Level 2 - Embedded Search

This is where things cranked up a notch. In late 2022 I discovered the OpenAI cookbook and started reverse engineering the experiments in there. I learned about vector embeddings and semantic search, and I decided that I’d love to have a search-augmented chatbot with all my textbooks and course material. I ended up building a full-stack app for this, called Docworks. The stack was Next.js/React + FastAPI + MongoDB + OpenAI API. 

This is where things cranked up a notch. In late 2022 I discovered the [OpenAI Cookbook](https://github.com/openai/openai-cookbook/tree/main/examples) and started reverse engineering the experiments in there. I learned about vector embeddings and semantic search, and I decided that I’d love to have a search-augmented chatbot with all my textbooks and course material. I ended up building a full-stack app for this, called Docworks. The stack was Next.js/React + FastAPI + MongoDB + OpenAI API. 

One day I was sent a journal article entitled *[Guidelines for Adolescent Depression in Primary Care (GLAD-PC): Part I. Practice Preparation, Identification, Assessment, and Initial Management](https://publications.aap.org/pediatrics/article/141/3/e20174081/37626/Guidelines-for-Adolescent-Depression-in-Primary?autologincheck=redirected)*, written in 2018 by the American Academy of Pediatrics. The article discusses new guidelines for the primary care physician’s management of adolescent depression (citation). I have a great interest in mental healthcare, and I always prefer to try and solve new use cases and create new applications of tech. I had the idea to build a Clinical Decision Support Tool for pediatricians that would help them screen adolescents for depression. I never tested this on human subject data (because it’s a hassle to get) so it ended up being a proof of concept. I’ve got a full write-up on this project (here).

<span id='level-3'/>
### Level 3 - Complex Chains and Code Execution

This is where things got interesting. By this point, I had become well acquainted with all the drawbacks of LLMs in production: hallucinations, limited training dataset, lack of access to external resources, inability to calculate, and so on. I had some grad school assignments where I had to write short (~1000 word) reviews of various literature. I wondered if I could write a review like this using only GPT-3.5 and Python.  There were a few major problems: I needed the writing to be verifiably academically correct, accurately cited (no plagiarism), and include an actual synthesis of material, not just an empty summarization. I was able to address each of these problems by always providing source material, using metadata to generate citations, and building a complex “chain of thought” pipeline that slowly generated and aggregated writing. The [final script takes a PDF document and writes a full review in about 2-2.5 minutes](https://github.com/beverm2391/ai-summarizer). I wrote a full write-up on this project (here).

Then I went to work on another clinical decision support tool, this time designed to help clinical geneticists pick which tests to order for their patients. I never finished this one, but I was able to build a pipeline that searched for an answer in a local database of genetic phenotypes, then dynamically wrote code to query the OMIM genomic database with a custom package, then returned the answer and updated the database.

Next, I wanted to automate my linear programming homework for my MBA Econ class. I built a small pipeline/chain that used GPT-4 to extract relevant function arguments from a word problem, parse them, run them through a solver function, and then interpret the solver’s answer back in the original context of the problem. Very soon after, OpenAI released its functions feature.

Lastly, since chatGPT was limiting the number of GPT-4 requests, I built my own version of ChatGPT’s frontend with streaming, latex rendering, and code syntax highlighting that runs off the GPT-4 API.  I also added semantic search, but it’s currently unstable. While I don’t use this much anymore, I’ve got a nice boilerplate for future projects. The stack is Next.js/React and FastAPI.

## How I Use LLMs Today

Right now, chatGPT does most of what I need, as it’s [significantly more accurate than GPT-3.5 with code completion](https://arxiv.org/abs/2303.08774). I also like [cursor](https://www.cursor.so/), a new IDE that more seamlessly integrates GPT-4 into the development workflow. I am a big fan of GitHub Copilot and use it every day. Anecdotally, I have noticed some significant performance increases in Copilot compared to the original public release.

## Open Source Models

The open-source community has been doing some incredible work with LLMs. I’m not going to list every model and benchmark that has been released - just know that the performance of Open Source models is rapidly improving. The best place to keep up with the daily Open Source innovations is Twitter/X. Also, inference is getting faster and faster due to some key lower-level innovations. I highly recommend checking out [ggml](https://ggml.ai/), [DeepSpeed](https://github.com/microsoft/DeepSpeed), [DeepSparse](https://github.com/neuralmagic/deepsparse), and [tinygrad](https://github.com/tinygrad/tinygrad).

That’s a synopsis of my journey with LLMs. I have learned so much from reverse engineering other’s code, so if you decide you want to go through mine, (here is the link to my main repo) called `benlp`. Feel free to (reach out with questions).

## Appendix 

<span id="oai-timeline"/>
### OpenAI Release Timeline
**2020**
- [Language Models are Few-Shot Learners](https://arxiv.org/abs/2005.14165)

**2021**
- [OpenAI’s API now available with no waitlist](https://openai.com/blog/api-no-waitlist)
- [OpenAI Codex](https://openai.com/blog/openai-codex)

**2022**
- [Text and Code Embeddings by Contrastive Pre-Training](https://arxiv.org/abs/2201.10005)
- [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155)
- [Introducing ChatGPT](https://openai.com/blog/chatgpt)
- [New and improved embedding model](https://openai.com/blog/new-and-improved-embedding-model)

**2023**
- [Introducing ChatGPT and Whisper APIs](https://openai.com/blog/introducing-chatgpt-and-whisper-apis)
- [GPT-4 Technical Report](https://arxiv.org/abs/2303.08774)
- [GPT-4 API general availability](https://openai.com/blog/gpt-4-api-general-availability)

## References