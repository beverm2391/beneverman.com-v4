---
title: My Journey with Large Language Models
description: An overview of my journey with LLMs.
date: "2023-09-08"
authors:
  - beneverman
---

<Callout>All code is [here](https://github.com/beverm2391/benlp). Want a quick read? <InternalLink href='#level-3'>Skip to the coolest stuff.</InternalLink></Callout>

My first ever Python script was a cron job that periodically emailed me new PubMed articles related to a medicine I was on. My second was a script that sent me a push notification every time a piece of rare climbing gear was posted for sale on Mountain Project. In early 2022, I read GPT-3’s inaugural paper, [Language Models Are Few Shot Learners](https://arxiv.org/abs/2005.14165) (2020), and was awakened to the potential of leveraging Large Language Models (LLMs) to extend the complexity and capability of my projects. Here I’m going to lay out a timeline of the projects I’ve built using LLMs - their successes, limitations, and general usefulness today. I've consolidated my reusable code into a package called [benlp](https://github.com/beverm2391/benlp) (Ben's Natural Language Processing Library).

## My Projects

### Level 1 - Out Of The Box

Before GPT-3 was instruction tuned, the lowest hanging fruit in 2022 was using text-davinci-002 to assist in writing. At this point,even with precise prompting, the model struggled to write coherently out of the box. Then I wrote a script that would take a PDF or PowerPoint and generate bulleted notes. After I built this, I realized that I could code during all my lectures and just work off my generated summaries. Then I experimented with outputting notes in a format that I could batch import to Quizlet flashcards. Due to [supervised fine-tuning but a lack of reinforcement learning](https://arxiv.org/abs/2303.10420) text-davinci-002 had difficulty producing a consistently formatted output. 

### Level 2 - Embedded Search

This is where things cranked up a notch. In late 2022 I discovered the [OpenAI Cookbook](https://github.com/openai/openai-cookbook/tree/main/examples) and started reverse engineering the experiments in there. I learned about vector embeddings and semantic search, and I decided that I’d love to have a search-augmented chatbot with all my textbooks and course material. I ended up building a full-stack app for this, called Docworks. The stack was Next.js/React + FastAPI + MongoDB + OpenAI API. 

One day I was sent a journal article entitled *[Guidelines for Adolescent Depression in Primary Care (GLAD-PC): Part I. Practice Preparation, Identification, Assessment, and Initial Management](https://publications.aap.org/pediatrics/article/141/3/e20174081/37626/Guidelines-for-Adolescent-Depression-in-Primary?autologincheck=redirected)*, written in 2018 by the American Academy of Pediatrics. The article discusses new guidelines for the primary care physician’s management of adolescent depression (Zuckerbrot et al, 2018). I have a great interest in mental healthcare, and I always prefer to try and solve new use cases and create new applications of tech. I thought to build a Clinical Decision Support Tool for pediatricians that would help them screen adolescents for depression. I never tested this on human subject data (because it’s a hassle to get) so it ended up being a proof of concept. I’ve got a <InternalLink href="/blog/nlp-cdst">full write-up</InternalLink> on this project.

<span id='level-3'/>
### Level 3 - Complex Chains and Code Execution

This is where things got interesting. By this point, I had become well acquainted with all the drawbacks of LLMs in production: hallucinations, limited training dataset, lack of access to external resources, inability to calculate, and so on. I had some grad school assignments where I had to write short (~1000 word) reviews of various literature. I wondered if I could write a review like this using only GPT-3.5 (the newest model at the time) and Python.  There were a few notable difficulties: I needed the writing to be verifiably academically correct, accurately cited (no plagiarism), and include an actual synthesis of material, not just mere summarization. I was able to address each of these problems by always providing source material, using metadata to generate citations, and building a complex “chain of thought” pipeline that slowly generated and aggregated writing. The takes a PDF document and writes a full review in about 2-2.5 minutes. I wrote a <InternalLink href="/blog/ai-summarizer">full write-up</InternalLink> on this project.

Then I went to work on another clinical decision support tool, this time designed to help clinical geneticists pick which tests to order for their patients. I never finished this one, but I was able to build a pipeline that searched for an answer in a local database of genetic phenotypes, then dynamically wrote code to query the OMIM genomic database with a custom package, then returned the answer and updated the database.

Next, I wanted to automate my homework for my MBA Econ class. I built a small pipeline/chain that used GPT-4 to extract relevant function arguments from a word problem, parse them, run them through a solver function, and then interpret the solver’s answer back in the original context of the problem. Very soon after, OpenAI released its functions feature.

Lastly, since chatGPT was limiting the number of GPT-4 requests, I built my own version of ChatGPT’s frontend with streaming, latex rendering, and code syntax highlighting that runs off the GPT-4 API.  I also added semantic search, but it’s currently unstable. While I don’t use this much anymore, I’ve got a nice boilerplate for future projects. The stack is Next.js/React and FastAPI.

## How I Use LLMs Today

Right now, chatGPT does most of what I need, as it’s [significantly more accurate than GPT-3.5 with code generation](https://arxiv.org/abs/2303.08774). I also like [cursor](https://www.cursor.so/), a new IDE that more seamlessly integrates GPT-4 into the development workflow. I am a big fan of GitHub Copilot and use it every day. Anecdotally, I have noticed some significant performance increases in Copilot compared to the original public release.

## Open Source Models

The open-source community has been doing some incredible work with LLMs. I’m not going to list every model and benchmark that has been released - just know that the performance of Open Source models is rapidly improving. The best place to keep up with the daily Open Source innovations is Twitter/X. Also, inference is getting faster and faster due to some key lower-level innovations. I highly recommend checking out [ggml](https://ggml.ai/), [DeepSpeed](https://github.com/microsoft/DeepSpeed), [DeepSparse](https://github.com/neuralmagic/deepsparse), and [tinygrad](https://github.com/tinygrad/tinygrad).

That's an overview of my journey with LLMs thus far. I have learned the most from training my own models and reverse engineering code - [feel free to explore mine](https://github.com/beverm2391/benlp). Also feel free to reach out with questions on [Twitter](https://twitter.com/beneverman) or [email](mailto:evermanben@gmail.com).

## References

<span className='p-2'/>
1. Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., et al. (2020). Language Models are Few-Shot Learners. <a href="https://doi.org/10.48550/arXiv.2005.14165">arXiv:2005.14165</a>.
2. OpenAI. (2023). GPT-4 Technical Report. <a href="https://doi.org/10.48550/arXiv.2303.08774">arXiv:2303.08774</a>.
3. Ye, J., Chen, X., Xu, N., Zu, C., Shao, Z., Liu, S., et al. (2023). A Comprehensive Capability Analysis of GPT-3 and GPT-3.5 Series Models. <a href="https://doi.org/10.48550/arXiv.2303.10420">arXiv:2303.10420</a>.
4. Zuckerbrot, R. A., Cheung, A., Jensen, P. S., Stein, R. E. K., Laraque, D., et al. (2018). Guidelines for Adolescent Depression in Primary Care (GLAD-PC): Part I. Practice Preparation, Identification, Assessment, and Initial Management. Pediatrics, 141(3). <a href="https://doi.org/10.1542/peds.2017-4081">doi:10.1542/peds.2017-4081</a>.